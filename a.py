import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

# ==============================
# CONFIGURACI√ìN GLOBAL
# ==============================
RANDOM_SEED = 42
TEST_SIZE = 0.3
np.random.seed(RANDOM_SEED)

# ==============================
# 1. CARGA DE DATOS
# ==============================
df = pd.read_csv("creditcard.csv")

print("="*70)
print("AN√ÅLISIS EXPLORATORIO INICIAL")
print("="*70)
print(f"Shape original del dataset: {df.shape}")
print(f"N√∫mero de transacciones totales: {len(df):,}")
print(f"N√∫mero de fraudes: {df['Class'].sum():,}")
print(f"\nDistribuci√≥n de clases:")
print(df["Class"].value_counts())
print(f"\nPorcentaje de fraudes: {df['Class'].mean()*100:.3f}%")
print(f"Ratio de desbalance: {(df['Class']==0).sum()/(df['Class']==1).sum():.0f}:1")

# ==============================
# 2. PREPROCESAMIENTO
# ==============================
print("\n" + "="*70)
print("PREPROCESAMIENTO")
print("="*70)

# Separar variables predictoras y variable objetivo
X = df.drop(columns=["Class"])
y = df["Class"]

# Escalado de todas las variables num√©ricas
print("\n1. Aplicando StandardScaler a todas las caracter√≠sticas...")
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
print(f"   ‚úì {X_scaled.shape[1]} caracter√≠sticas escaladas (media=0, std=1)")

# Divisi√≥n en train/test (70/30)
print("\n2. Divisi√≥n train/test (70%/30%) con estratificaci√≥n...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED
)

print(f"   ‚úì Train: {X_train.shape[0]:,} muestras")
print(f"   ‚úì Test:  {X_test.shape[0]:,} muestras")
print(f"\nDistribuci√≥n en conjunto de entrenamiento:")
print(f"  - Clase 0 (Normal): {(y_train==0).sum():,} ({(y_train==0).mean()*100:.2f}%)")
print(f"  - Clase 1 (Fraude): {(y_train==1).sum():,} ({(y_train==1).mean()*100:.2f}%)")
print(f"  - Ratio de desbalance: {(y_train==0).sum()/(y_train==1).sum():.0f}:1")

# ==============================
# 3. EXPERIMENTO: JUSTIFICACI√ìN DEL N√öMERO DE CARACTER√çSTICAS (k)
# ==============================
print("\n" + "="*70)
print("EXPERIMENTO: SELECCI√ìN √ìPTIMA DE k")
print("="*70)
print("\nObjetivo: Determinar el n√∫mero √≥ptimo de caracter√≠sticas a seleccionar")
print("M√©todo: Validaci√≥n cruzada con diferentes valores de k")
print("M√©trica: Recall (Sensibilidad) - prioritaria para detecci√≥n de fraude")

# Valores de k a probar
k_values = [5, 10, 15, 20, 25]
results_k = []

# Clasificador base para evaluar (r√°pido y robusto)
base_clf = RandomForestClassifier(
    n_estimators=50, 
    max_depth=10,
    class_weight='balanced',  # Maneja desbalance sin SMOTE para esta evaluaci√≥n
    random_state=RANDOM_SEED,
    n_jobs=-1
)

print("Evaluando k = ", end="", flush=True)
for k in k_values:
    print(f"{k}...", end=" ", flush=True)
    
    # Seleccionar k caracter√≠sticas
    selector_temp = SelectKBest(score_func=mutual_info_classif, k=k)
    X_train_k = selector_temp.fit_transform(X_train, y_train)
    
    # Validaci√≥n cruzada estratificada (3-fold por eficiencia)
    cv_scores = cross_val_score(
        base_clf, X_train_k, y_train,
        cv=3,
        scoring='recall',  # Prioridad: detectar fraudes
        n_jobs=-1
    )
    
    results_k.append({
        'k': k,
        'recall_mean': cv_scores.mean(),
        'recall_std': cv_scores.std(),
        'cv_scores': cv_scores
    })

print("‚úì\n")

# Mostrar resultados
print("-"*70)
print("RESULTADOS: Desempe√±o por n√∫mero de caracter√≠sticas")
print("-"*70)
print(f"{'k':<6} {'Recall (Media)':<18} {'Recall (Std)':<15} {'Evaluaci√≥n'}")
print("-"*70)

best_result = max(results_k, key=lambda x: x['recall_mean'])
OPTIMAL_K = best_result['k']

for res in results_k:
    marker = " ‚Üê √ìPTIMO" if res['k'] == OPTIMAL_K else ""
    print(f"{res['k']:<6} {res['recall_mean']:.4f}           "
          f"{res['recall_std']:.4f}          {marker}")

print("\n" + "="*70)
print("JUSTIFICACI√ìN DE k SELECCIONADO")
print("="*70)
print(f"\n‚úÖ Valor √≥ptimo: k = {OPTIMAL_K}")
print(f"\nüìä Razones:")
print(f"   1. Mayor sensibilidad promedio: {best_result['recall_mean']:.4f}")
print(f"   2. Desviaci√≥n est√°ndar aceptable: {best_result['recall_std']:.4f}")
print(f"   3. Reducci√≥n de dimensionalidad: {100*(1-OPTIMAL_K/30):.0f}%")
print(f"   4. Balance entre complejidad y desempe√±o")

# Guardar resultados del experimento
experiment_df = pd.DataFrame(results_k)[['k', 'recall_mean', 'recall_std']]
experiment_df.to_csv("k_selection_experiment.csv", index=False)
print(f"\n‚úì Resultados guardados en: k_selection_experiment.csv")

# ==============================
# 4. SELECCI√ìN DE CARACTER√çSTICAS CON k √ìPTIMO
# ==============================
print("\n" + "="*70)
print("SELECCI√ìN DE CARACTER√çSTICAS (k={})".format(OPTIMAL_K))
print("="*70)
print(f"\n4. Aplicando SelectKBest con k={OPTIMAL_K}...")
print(f"   T√©cnica: Selecci√≥n basada en tests estad√≠sticos")
print(f"   M√©trica: Informaci√≥n Mutua (detecta relaciones no lineales)")
print(f"   IMPORTANTE: Se aplica ANTES del balanceo para evitar sesgo\n")

# SelectKBest con k √≥ptimo
selector = SelectKBest(score_func=mutual_info_classif, k=OPTIMAL_K)
selector.fit(X_train, y_train)

# Obtener caracter√≠sticas seleccionadas
selected_features = X_train.columns[selector.get_support()].tolist()

# Mostrar scores
scores = pd.DataFrame({
    'Feature': X_train.columns,
    'Score': selector.scores_
}).sort_values('Score', ascending=False)

print("="*70)
print("VECTOR DE CARACTER√çSTICAS SELECCIONADAS")
print("="*70)
for i, feature in enumerate(selected_features, start=1):
    score = scores[scores['Feature'] == feature]['Score'].values[0]
    print(f"  {i:2d}. {feature:6s}  (Score MI: {score:.4f})")

# ==============================
# 5. BALANCEO DE CLASES (SMOTE)
# ==============================
print("\n" + "="*70)
print("ESTRATEGIA DE BALANCEO")
print("="*70)
print("\n5. Aplicando SMOTE (Synthetic Minority Over-sampling Technique)...")
print("   Estrategia: Sobremuestreo de la clase minoritaria (fraudes)")
print("   ‚ö†Ô∏è  CR√çTICO: Se aplica SOLO sobre entrenamiento (no validaci√≥n ni test)")
print("   Nota: Se aplica DESPU√âS de selecci√≥n de caracter√≠sticas\n")

smote = SMOTE(random_state=RANDOM_SEED)
X_train_bal, y_train_bal = smote.fit_resample(X_train[selected_features], y_train)

print("Distribuci√≥n DESPU√âS del balanceo:")
print(f"  - Clase 0 (Normal): {(y_train_bal==0).sum():,} ({(y_train_bal==0).mean()*100:.2f}%)")
print(f"  - Clase 1 (Fraude): {(y_train_bal==1).sum():,} ({(y_train_bal==1).mean()*100:.2f}%)")
print(f"  - Nuevo ratio: {(y_train_bal==0).sum()/(y_train_bal==1).sum():.1f}:1")
print("   ‚úì Dataset balanceado correctamente")

# ==============================
# GUARDAR RESULTADOS
# ==============================
print("\n" + "="*70)
print("RESUMEN Y GUARDADO DE ARCHIVOS")
print("="*70)

print(f"\n‚úì Escalado:          StandardScaler aplicado")
print(f"‚úì Divisi√≥n:          70% train ({X_train.shape[0]:,}) / 30% test ({X_test.shape[0]:,})")
print(f"‚úì Selecci√≥n k:       Justificado experimentalmente (k={OPTIMAL_K})")
print(f"‚úì Selecci√≥n:         SelectKBest con Mutual Information")
print(f"‚úì Caracter√≠sticas:   {OPTIMAL_K} seleccionadas de {X_scaled.shape[1]} originales")
print(f"‚úì Balanceo:          SMOTE aplicado SOLO en train")

print(f"\nDimensiones finales:")
print(f"  - X_train_bal: {X_train_bal.shape}  ‚Üê CON datos sint√©ticos")
print(f"  - y_train_bal: {y_train_bal.shape}")
print(f"  - X_test:      {X_test[selected_features].shape}  ‚Üê SIN datos sint√©ticos")
print(f"  - y_test:      {y_test.shape}")

pd.Series(selected_features).to_csv("selected_features.csv", index=False, header=False)
X_train_bal.to_csv("X_train_bal.csv", index=False)
y_train_bal.to_csv("y_train_bal.csv", index=False, header=True)
X_test[selected_features].to_csv("X_test.csv", index=False)
y_test.to_csv("y_test.csv", index=False, header=True)

print("\n‚úì Archivos guardados exitosamente:")
print("  - k_selection_experiment.csv  ‚Üê NUEVO: justificaci√≥n de k")
print("  - selected_features.csv")
print("  - X_train_bal.csv / y_train_bal.csv  ‚Üê CON SMOTE")
print("  - X_test.csv / y_test.csv            ‚Üê SIN SMOTE")

print("\n" + "="*70)
print("‚úÖ PREPROCESAMIENTO COMPLETADO CORRECTAMENTE")
print("="*70)